{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"development.json\") as datafile:\n",
    "  data = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import InferSent\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "W2V_PATH = 'dataset/fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)\n",
    "infersent.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "training_json = {}\n",
    "development_json = {}\n",
    "test_json = {}\n",
    "training_features = {}\n",
    "training_labels = {}\n",
    "feats = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\", \"what\", \"when\", \"how\", \"many\", \"why\", \"who\", 'percentage NNP', 'percentage NNPS', 'percentage CD']\n",
    "\n",
    "training_dict = {}\n",
    "development_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "####TRAINING PREPROCESSING - Creating feature vectors\n",
    "with open('training.json','r') as f:\n",
    "    training_json = json.load(f)\n",
    "\n",
    "for x in training_json['data']:\n",
    "    for p in x['paragraphs']:\n",
    "        context = p['context']\n",
    "        doc = nlp(context)\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        features = defaultdict(int)\n",
    "        features = Counter(labels)\n",
    "        questions = {}\n",
    "        for q in p['qas']:\n",
    "            qid = q['id']\n",
    "            questions[qid] = {q['question']:q['is_impossible']}\n",
    "            words = nltk.word_tokenize(q['question'])\n",
    "            features['what'] = int(any((True for word in words if word.lower() == \"what\")))\n",
    "            features['when'] = int(any((True for word in words if word.lower() == \"when\")))\n",
    "            features['where'] = int(any((True for word in words if word.lower() == \"where\")))\n",
    "            features['how'] = int(any((True for word in words if word.lower() == \"how\")))\n",
    "            features['many'] = int(any((True for word in words if word.lower() == \"many\")))\n",
    "            features['why'] = int(any((True for word in words if word.lower() == \"why\")))\n",
    "            features['who'] = int(any((True for word in words if word.lower() == \"who\")))\n",
    "            \n",
    "            tagged = nltk.pos_tag(words)\n",
    "            percentPronouns = 0\n",
    "            percentNouns = 0\n",
    "            percentNumbers = 0\n",
    "            #print(tagged)\n",
    "            properNouns = [word for word,pos in tagged if pos == 'NNP']\n",
    "            numPN = len(properNouns)\n",
    "            countPN = 0\n",
    "            if numPN > 0: \n",
    "                for pn in properNouns:\n",
    "                    if pn in context:   \n",
    "                        countPN = countPN + 1\n",
    "                percentPronouns = float(countPN)/numPN\n",
    "            \n",
    "            nouns = [word for word,pos in tagged if pos == 'NN']\n",
    "            numNN = len(nouns)\n",
    "            countNN = 0\n",
    "            if numNN > 0:\n",
    "                for nn in nouns:\n",
    "                    if nn in context:\n",
    "                        countNN = countNN + 1\n",
    "                percentNouns = float(countNN)/numNN\n",
    "               \n",
    "            numbers = [word for word,pos in tagged if pos == 'CD']\n",
    "            numNumbers = len(numbers)\n",
    "            countNumbers = 0\n",
    "            if numNumbers > 0:\n",
    "                for num in numbers: \n",
    "                    if num in context:\n",
    "                        countNumbers = countNumbers + 1\n",
    "                percentNumbers = float(countNumbers)/numNumbers\n",
    "            \n",
    "            features['percentage NNP'] = percentPronouns\n",
    "            features['percentage NN'] = percentNouns\n",
    "            features['percentage CD'] = percentNumbers\n",
    "            \n",
    "            feature_vector = []\n",
    "            for i in feats:\n",
    "                feature_vector.append(features[i])\n",
    "            training_features[qid] = feature_vector\n",
    "            training_labels[qid] = int(q['is_impossible'])\n",
    "            \n",
    "        training_dict[context] = questions\n",
    "        \n",
    "with open('training_features.json', 'w') as fp:\n",
    "    json.dump(training_features, fp)\n",
    "with open('training_labels.json', 'w') as fp:\n",
    "    json.dump(training_labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLANNED SENTENCE EMBEDDING\n",
    "sentence_dict = {}\n",
    "sentences = []\n",
    "for x in training_json['data']:\n",
    "    for p in x['paragraphs']:\n",
    "        context = p['context']\n",
    "        sentence = sent_tokenize(context)\n",
    "        for sent in sentence:\n",
    "          sentences.append(sent)\n",
    "        for q in p['qas']:\n",
    "          question = q['question']\n",
    "          sentences.append(question)\n",
    "for i in range(len(sentences)):\n",
    "    sentence_dict[sentences[i]] = infersent.encode([sentences[i]], tokenize=True)\n",
    "\n",
    "with open('sentence_embed.json', 'w') as fp:\n",
    "    json.dump(sentence_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####DEVELOPMENT PREPROCESSING\n",
    "development_features = {}\n",
    "test_features = {}\n",
    "\n",
    "with open('development.json','r') as f:\n",
    "    development_json = json.load(f)\n",
    "\n",
    "for x in development_json['data']:\n",
    "    for p in x['paragraphs']:\n",
    "        context = p['context']\n",
    "        doc = nlp(context)\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        features = defaultdict(int)\n",
    "        features = Counter(labels)\n",
    "        questions = {}\n",
    "        for q in p['qas']:\n",
    "            qid = q['id']\n",
    "            questions[qid] = {q['question']:q['is_impossible']}\n",
    "            words = nltk.word_tokenize(q['question'])\n",
    "            features['what'] = int(any((True for word in words if word.lower() == \"what\")))\n",
    "            features['when'] = int(any((True for word in words if word.lower() == \"when\")))\n",
    "            features['where'] = int(any((True for word in words if word.lower() == \"where\")))\n",
    "            features['how'] = int(any((True for word in words if word.lower() == \"how\")))\n",
    "            features['many'] = int(any((True for word in words if word.lower() == \"many\")))\n",
    "            features['why'] = int(any((True for word in words if word.lower() == \"why\")))\n",
    "            features['who'] = int(any((True for word in words if word.lower() == \"who\")))\n",
    "            \n",
    "            tagged = nltk.pos_tag(words)\n",
    "            percentPronouns = 0\n",
    "            percentNouns = 0\n",
    "            percentNumbers = 0\n",
    "            #print(tagged)\n",
    "            properNouns = [word for word,pos in tagged if pos == 'NNP']\n",
    "            numPN = len(properNouns)\n",
    "            countPN = 0\n",
    "            if numPN > 0: \n",
    "                for pn in properNouns:\n",
    "                    if pn in context:   \n",
    "                        countPN = countPN + 1\n",
    "                percentPronouns = float(countPN)/numPN\n",
    "            \n",
    "            nouns = [word for word,pos in tagged if pos == 'NN']\n",
    "            numNN = len(nouns)\n",
    "            countNN = 0\n",
    "            if numNN > 0:\n",
    "                for nn in nouns:\n",
    "                    if nn in context:\n",
    "                        countNN = countNN + 1\n",
    "                percentNouns = float(countNN)/numNN\n",
    "               \n",
    "            numbers = [word for word,pos in tagged if pos == 'CD']\n",
    "            numNumbers = len(numbers)\n",
    "            countNumbers = 0\n",
    "            if numNumbers > 0:\n",
    "                for num in numbers: \n",
    "                    if num in context:\n",
    "                        countNumbers = countNumbers + 1\n",
    "                percentNumbers = float(countNumbers)/numNumbers\n",
    "            \n",
    "            features['percentage NNP'] = percentPronouns\n",
    "            features['percentage NN'] = percentNouns\n",
    "            features['percentage CD'] = percentNumbers\n",
    "            \n",
    "            feature_vector = []\n",
    "            for i in feats:\n",
    "                feature_vector.append(features[i])\n",
    "            development_features[qid] = feature_vector\n",
    "            \n",
    "        development_dict[context] = questions\n",
    "\n",
    "\n",
    "with open('development_features.json', 'w') as fp:\n",
    "    json.dump(development_features, fp)\n",
    "\n",
    "\n",
    "####TESTING\n",
    "with open('test.json','r') as f:\n",
    "    test_json = json.load(f)\n",
    "\n",
    "for x in test_json['data']:\n",
    "    for p in x['paragraphs']:\n",
    "        context = p['context']\n",
    "        doc = nlp(context)\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        features = defaultdict(int)\n",
    "        features = Counter(labels)\n",
    "        questions = {}\n",
    "        for q in p['qas']:\n",
    "            qid = q['id']\n",
    "            questions[qid] = q['question']\n",
    "            words = nltk.word_tokenize(q['question'])\n",
    "            features['what'] = int(any((True for word in words if word.lower() == \"what\")))\n",
    "            features['when'] = int(any((True for word in words if word.lower() == \"when\")))\n",
    "            features['where'] = int(any((True for word in words if word.lower() == \"where\")))\n",
    "            features['how'] = int(any((True for word in words if word.lower() == \"how\")))\n",
    "            features['many'] = int(any((True for word in words if word.lower() == \"many\")))\n",
    "            features['why'] = int(any((True for word in words if word.lower() == \"why\")))\n",
    "            features['who'] = int(any((True for word in words if word.lower() == \"who\")))\n",
    "            \n",
    "            tagged = nltk.pos_tag(words)\n",
    "            percentPronouns = 0\n",
    "            percentNouns = 0\n",
    "            percentNumbers = 0\n",
    "            #print(tagged)\n",
    "            properNouns = [word for word,pos in tagged if pos == 'NNP']\n",
    "            numPN = len(properNouns)\n",
    "            countPN = 0\n",
    "            if numPN > 0: \n",
    "                for pn in properNouns:\n",
    "                    if pn in context:   \n",
    "                        countPN = countPN + 1\n",
    "                percentPronouns = float(countPN)/numPN\n",
    "            \n",
    "            nouns = [word for word,pos in tagged if pos == 'NN']\n",
    "            numNN = len(nouns)\n",
    "            countNN = 0\n",
    "            if numNN > 0:\n",
    "                for nn in nouns:\n",
    "                    if nn in context:\n",
    "                        countNN = countNN + 1\n",
    "                percentNouns = float(countNN)/numNN\n",
    "               \n",
    "            numbers = [word for word,pos in tagged if pos == 'CD']\n",
    "            numNumbers = len(numbers)\n",
    "            countNumbers = 0\n",
    "            if numNumbers > 0:\n",
    "                for num in numbers: \n",
    "                    if num in context:\n",
    "                        countNumbers = countNumbers + 1\n",
    "                percentNumbers = float(countNumbers)/numNumbers\n",
    "            \n",
    "            features['percentage NNP'] = percentPronouns\n",
    "            features['percentage NN'] = percentNouns\n",
    "            features['percentage CD'] = percentNumbers\n",
    "            \n",
    "            feature_vector = []\n",
    "            for i in feats:\n",
    "                feature_vector.append(features[i])\n",
    "            test_features[qid] = feature_vector\n",
    "        test_dict[context] = questions\n",
    "\n",
    "with open('test_features.json', 'w') as fp:\n",
    "    json.dump(test_features, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the root\n",
    "from nltk import Tree\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "\n",
    "import json\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "with open('training.json','r') as f:\n",
    "    training_json = json.load(f)\n",
    "\n",
    "question_root_dict = {} #dictionary for boolean of whether the root of the sentences is in the context \n",
    "for x in training_json['data']:\n",
    "    for p in x['paragraphs']:\n",
    "        sentences = []\n",
    "        context = p['context']\n",
    "        sentence = nlp(context.lower()).sents\n",
    "        for sent in sentence:\n",
    "          sentences.append(sent)\n",
    "        for q in p['qas']:\n",
    "          qid = q['id']\n",
    "          question = q['question']\n",
    "          question_root_dict[qid] = 0\n",
    "          question = question.lower()\n",
    "          qroot = st.stem(str([sent.root for sent in nlp(question).sents][0])) #if the question has 2 parts, just pick the first and stem it\n",
    "          for sent in sentences:\n",
    "            sroots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks] \n",
    "            if qroot in sroots:\n",
    "              question_root_dict[qid] = 1\n",
    "              break\n",
    "            \n",
    "with open('question_root_train.json', 'w') as fp:\n",
    "    json.dump(question_root_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reloading features\n",
    "with open('training_features.json','r') as f:\n",
    "    training_json = json.load(f)\n",
    "training_features = training_json\n",
    "\n",
    "with open('test_features.json','r') as f:\n",
    "    test_json = json.load(f)\n",
    "    \n",
    "test_features = test_json\n",
    "\n",
    "with open('question_root_train.json', 'r') as f:\n",
    "    question_root_dict_json = json.load(f)\n",
    "question_root_dict = question_root_dict_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 2 -> mixing NER counts and removing extraneous ones\n",
    "new_training_features = {}\n",
    "for key in training_features:\n",
    "    features = training_features[key]\n",
    "    new_features = []\n",
    "    new_features.append(features[0])\n",
    "    new_features.append(features[1]+features[3])\n",
    "    new_features.append(features[4]+features[5])\n",
    "    new_features.append(features[2]+features[6]+features[10])\n",
    "    new_features.append(features[11]+features[12])\n",
    "    new_features.append(features[13]+features[14]+features[15]+features[17])\n",
    "    for i in range(18,len(feats)):\n",
    "        new_features.append(features[i])\n",
    "    new_training_features[key] = new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt 3 - only keeping the last 3 features\n",
    "# new_training_features = {}\n",
    "# for key in training_features:\n",
    "#     features = training_features[key]\n",
    "#     new_features = features[-3:]\n",
    "#     new_training_features[key] = new_features\n",
    "\n",
    "# appending the root boolean\n",
    "for key in training_features:\n",
    "    new_training_features[key].append(question_root_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing feature vectors and gold standard labels for logistic regression\n",
    "X = []\n",
    "y = []\n",
    "for key in training_features:\n",
    "    X.append(new_training_features[key])\n",
    "    y.append(int(not training_labels[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implementation of Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(X)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating root boolean for test set\n",
    "with open('test.json','r') as f:\n",
    "    testing_json = json.load(f)\n",
    "\n",
    "question_root_dict_test = {}\n",
    "for x in testing_json['data']:\n",
    "    for p in x['paragraphs']:\n",
    "        sentences = []\n",
    "        context = p['context']\n",
    "        sentence = nlp(context.lower()).sents\n",
    "        for sent in sentence:\n",
    "          sentences.append(sent)\n",
    "        for q in p['qas']:\n",
    "          qid = q['id']\n",
    "          question = q['question']\n",
    "          question_root_dict_test[qid] = 0\n",
    "          question = question.lower()\n",
    "          qroot = st.stem(str([sent.root for sent in nlp(question).sents][0]))\n",
    "          for sent in sentences:\n",
    "            sroots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n",
    "            if qroot in sroots:\n",
    "              question_root_dict_test[qid] = 1\n",
    "              break\n",
    "            \n",
    "with open('question_root_test.json', 'w') as fp:\n",
    "    json.dump(question_root_dict_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching attempts 2+3 for the dev/test sets\n",
    "import csv\n",
    "\n",
    "new_dev_features = {}\n",
    "for key in development_features:\n",
    "    features = development_features[key]\n",
    "    new_features = []\n",
    "    new_features.append(features[0])\n",
    "    new_features.append(features[1]+features[3])\n",
    "    new_features.append(features[4]+features[5])\n",
    "    new_features.append(features[2]+features[6]+features[10])\n",
    "    new_features.append(features[11]+features[12])\n",
    "    new_features.append(features[13]+features[14]+features[15]+features[17])\n",
    "    for i in range(18,len(feats)):\n",
    "        new_features.append(features[i])\n",
    "    new_dev_features[key] = new_features\n",
    "    \n",
    "new_dev_features = {}\n",
    "for key in development_features:\n",
    "    features = development_features[key]\n",
    "    new_features = features[-3:]\n",
    "    new_dev_features[key] = new_features\n",
    "\n",
    "    \n",
    "# predictions = {}\n",
    "# dev_predictions = {}\n",
    "# for key in development_features:\n",
    "#     val = int(model.predict([new_dev_features[key]]))\n",
    "#     dev_predictions[key] = val\n",
    "\n",
    "# with open('dev_predictions.json', 'w') as fp:\n",
    "#     json.dump(dev_predictions, fp)\n",
    "\n",
    "new_test_features = {}\n",
    "for key in test_features:\n",
    "    features = test_features[key]\n",
    "    new_features = []\n",
    "    new_features.append(features[0])\n",
    "    new_features.append(features[1]+features[3])\n",
    "    new_features.append(features[4]+features[5])\n",
    "    new_features.append(features[2]+features[6]+features[10])\n",
    "    new_features.append(features[11]+features[12])\n",
    "    new_features.append(features[13]+features[14]+features[15]+features[17])\n",
    "    for i in range(18,len(feats)):\n",
    "        new_features.append(features[i])\n",
    "    new_test_features[key] = new_features\n",
    "\n",
    "# new_test_features = {}\n",
    "# for key in test_features:\n",
    "#     features = test_features[key]\n",
    "#     new_features = features[-3:]\n",
    "#     new_test_features[key] = new_features\n",
    "\n",
    "for key in test_features:\n",
    "    new_test_features[key].append(question_root_dict_test[key])\n",
    "    \n",
    "for key in test_features:\n",
    "    val = int(model.predict([new_test_features[key]]))\n",
    "    predictions[key] = val\n",
    "    \n",
    "# Creating prediction csv\n",
    "with open('preds_p4.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerow([\"Category\", \"Id\"])\n",
    "    for key in predictions:\n",
    "        spamwriter.writerow([predictions[key], key])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
